








!pip list


import numpy as np
#np.object = np.object_
from tensorflow import keras
from keras_preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img


# Linking directory path
train_data_dir = 'data/chest_xray/train'
val_data_dir = 'data/chest_xray/val'
test_data_dir = 'data/chest_xray/test'


datagen = ImageDataGenerator()


# Getting train, test, and validation data from directory
train_generator = ImageDataGenerator().flow_from_directory(train_data_dir,
                                                           target_size=(64, 64),
                                                           batch_size=5000)

test_generator = ImageDataGenerator().flow_from_directory(test_data_dir,
                                                          target_size=(64, 64),
                                                          batch_size=624)

val_generator = ImageDataGenerator().flow_from_directory(val_data_dir,
                                                         target_size=(64, 64),
                                                         batch_size=232)


# Creating datasets, takes about a minute to run.
train_images, train_labels = next(train_generator)
test_images, test_labels = next(test_generator)
val_images, val_labels = next(val_generator)


# Reshaping data
train_img_unrow = train_images.reshape(5000, -1)
test_img_unrow = test_images.reshape(624, -1)
val_img_unrow = val_images.reshape(232, -1)


# Checking data shape
print(np.shape(train_img_unrow))
print(np.shape(test_img_unrow))
print(np.shape(val_img_unrow))


# Normalizing data
train_img_final = train_img_unrow/255.0
test_img_final = test_img_unrow/255.0
val_img_final = val_img_unrow/255.0











np.shape(train_img_final)


train_img_final


type(train_labels)





from imblearn.over_sampling import SMOTE
from collections import Counter

train_img_resamp, train_labels_resamp = SMOTE().fit_resample(train_images, train_labels)
print(sorted(Counter(train_labels_resamp).items()))


import sklearn
print(sklearn.__version__)


!pip install --upgrade pmdarima











train_generator.class_indices


from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(class_weight='balanced', 
                                     classes=np.unique(train_labels_final), 
                                     y=train_labels)

# Convert the array to a dictionary mapping class index to weight
class_weights = dict(enumerate(class_weights_array)) 


# Reshaping data
train_img_unrow = train_images.reshape(5000, -1).T
test_img_unrow = test_images.reshape(624, -1).T
val_img_unrow = val_images.reshape(232, -1).T


print(np.shape(train_img_unrow))
print(np.shape(test_img_unrow))
print(np.shape(val_img_unrow))


train_labels_final = train_labels.T[[1]]
val_labels_final = val_labels.T[[1]]


print(np.shape(train_labels_final))
print(np.shape(val_labels_final))


type(train_labels_final)


array_to_img(train_images[50])


train_labels_final[:,50]


np.shape(val_labels_final)
































from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns

import tensorflow as tf
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau


# Function to quickly plot model results
def plot_results(model_results):
    """
    This function will take in the results of a model and plot the loss and accuracy of the model.

    Args:
    model_results (list): the results of a model in [] brackets.
    """
    for results in model_results:
        train_loss = results.history['loss']
        train_acc = results.history['accuracy']
        val_loss = results.history['val_loss']
        val_acc = results.history['val_accuracy']

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
        sns.lineplot(x=results.epoch, y=train_loss, ax=ax1, label='train_loss')
        sns.lineplot(x=results.epoch, y=train_acc, ax=ax2, label='train_accuracy')

        sns.lineplot(x=results.epoch, y=val_loss, ax=ax1, label='val_loss')
        sns.lineplot(x=results.epoch, y=val_acc, ax=ax2, label='val_accuracy')
        ax1.set_title('Loss')
        ax2.set_title('Accuracy')
        ax1.legend();


# Function to quickly print out model evaluation for mulitple data sets
def model_evaluation(model, data_dict):
    """
    This function takes in a sequential model as a well as a dictionary of data and 
    prints out the model evaluation results.
    
    Arguments:
    model: A trained model with .evaluate() capabilities
    data_dict: A dictionary where Keys are dataset names and Values are tuples (X, y) for data and labels. 
    """
    for name, (X, y) in data_dict.items():
        loss, acc = model.evaluate(X, y)
        
        print(f'{name}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')


# Creating dictionary for train, test, validation data
data_dict = {
    'Train Data': (train_images, train_labels),
    'Test Data': (test_images, test_labels),
    'Val Data': (val_images, val_labels)
}


# Creating dictionary for normalized train, test, validation data
data_dict_norm = {
    'Train Data': (train_img_final, train_labels),
    'Test Data': (test_img_final, test_labels),
    'Val Data': (val_img_final, val_labels)
}





# Model 1 base model
model_1 = Sequential()


model_1.add(layers.Input(shape=(12288,)))
model_1.add(Dense(50, activation='relu'))
model_1.add(Dense(25, activation='relu'))
model_1.add(Dense(10, activation='relu'))
model_1.add(Dense(2, activation='softmax'))


model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


model_1_results = model_1.fit(train_img_final, train_labels,
                              epochs=10,
                              batch_size=125, 
                              validation_data=(val_img_final, val_labels))


plot_results([model_1_results])


model_evaluation(model_1, data_dict_norm)





# Model 2
model_2 = Sequential()


model_2.add(layers.Input(shape=(12288,)))
model_2.add(Dense(50, activation='relu'))
model_2.add(Dense(25, activation='relu'))
model_2.add(Dropout(0.5))
model_2.add(Dense(10, activation='relu'))
model_2.add(Dropout(0.2))
model_2.add(Dense(2, activation='softmax'))


model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


model_2_results = model_2.fit(train_img_final, train_labels,
                              epochs=50,
                              batch_size=300, 
                              validation_data=(val_img_final, val_labels))


plot_results([model_2_results])


model_evaluation(model_2, data_dict_norm)





# Creating an earlystop variable
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


model_3 = Sequential()


model_3.add(layers.Input(shape=(12288,)))
model_3.add(Dense(50, activation='relu'))
model_3.add(Dense(50, activation='relu', kernel_regularizer=l2(l2=0.5)))
model_3.add(Dropout(0.5))
model_3.add(Dense(25, activation='relu'))
model_3.add(Dense(25, activation='relu', kernel_regularizer=l2(l2=0.25)))
model_3.add(Dropout(0.2))
model_3.add(Dense(10, activation='relu'))
model_3.add(Dense(2, activation='softmax'))


model_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


model_3_results = model_3.fit(train_img_final, train_labels,
                              epochs=40, 
                              batch_size=350,
                              validation_data=(val_img_final, val_labels),
                              callbacks=[early_stop])


plot_results([model_3_results])


model_evaluation(model_3, data_dict_norm)





model_4 = Sequential()


model_4.add(layers.Input(shape=(12288,)))
model_4.add(Dense(75, activation='relu'))
model_4.add(Dense(50, activation='relu'))
model_4.add(Dropout(0.5))
model_4.add(Dense(50, activation='relu', kernel_regularizer=l2(l2=0.5)))
model_4.add(Dense(25, activation='relu'))
model_4.add(Dropout(0.25))
model_4.add(Dense(25, activation='relu', kernel_regularizer=l2(l2=0.25)))
model_4.add(Dense(10, activation='relu', kernel_regularizer=l2(l2=0.01)))
model_4.add(Dense(2, activation='softmax'))


model_4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])


model_4_results = model_4.fit(train_img_final, train_labels,
                              epochs=50,
                              batch_size=250,
                              validation_data=(val_img_final, val_labels),
                              callbacks=[early_stop])


plot_results([model_4_results])


model_evaluation(model_4, data_dict_norm)





# reduce learning rate for val_accuracy
reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.3, patience=2, min_lr=1e-5)


model_5 = models.Sequential()


model_5.add(layers.Conv2D(16, (5, 5), activation='relu', input_shape=(64, 64, 3)))
model_5.add(layers.BatchNormalization())
model_5.add(layers.MaxPooling2D())

model_5.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
model_5.add(layers.BatchNormalization())

model_5.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2=0.00001)))
model_5.add(layers.BatchNormalization())
model_5.add(layers.Dropout(0.5))
model_5.add(layers.MaxPooling2D())

model_5.add(layers.Conv2D(64, (3, 3), activation='relu', padding='valid'))

model_5.add(layers.Conv2D(64, (3, 3), activation='relu', padding='valid'))
model_5.add(layers.MaxPooling2D())

model_5.add(layers.Conv2D(128, (3, 3), activation='relu', padding='valid', kernel_regularizer=l2(l2=0.0001)))
model_5.add(layers.Dropout(0.1))
model_5.add(layers.MaxPooling2D())

model_5.add(layers.Flatten())
model_5.add(layers.Dense(2, activation='softmax'))


model_5.compile(Adam(learning_rate=0.0002), loss='binary_crossentropy', metrics=['accuracy'])


model_5_results = model_5.fit(train_images, train_labels,
                              epochs=20,
                              batch_size=340,
                              validation_data=(val_images, val_labels),
                              callbacks=[early_stop, reduce_lr])


plot_results([model_5_results])


model_evaluation(model_5, data_dict)








final_model = Sequential()


final_model.add(layers.Conv2D(16, (5, 5), activation='relu', input_shape=(64, 64, 3)))
final_model.add(layers.BatchNormalization())
final_model.add(layers.MaxPooling2D())

final_model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))
final_model.add(layers.BatchNormalization())

final_model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(l2=0.00001)))
final_model.add(layers.BatchNormalization())
final_model.add(layers.Dropout(0.5))
final_model.add(layers.MaxPooling2D())

final_model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='valid'))

final_model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='valid'))
final_model.add(layers.MaxPooling2D())

final_model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='valid', kernel_regularizer=l2(l2=0.0001)))
final_model.add(layers.Dropout(0.1))
final_model.add(layers.MaxPooling2D())

final_model.add(layers.Flatten())
final_model.add(layers.Dense(2, activation='softmax'))


final_model.compile(Adam(learning_rate=0.0002), loss='binary_crossentropy', metrics=['accuracy'])


final_model_results = final_model.fit(train_images, train_labels,
                                      epochs=20,
                                      batch_size=340,
                                      validation_data=(val_images, val_labels),
                                      callbacks=[early_stop, reduce_lr] 
                                      )


plot_results([final_model_results])


model_evaluation(final_model, data_dict)


test_img_pred = final_model.predict(test_images)


# Confusion Matrix of final Model
fig, ax = plt.subplots(figsize=(15, 10))

ax.set_title('Final Model Results')

test_img_pred_labels = np.argmax(test_img_pred, axis=1)
test_true_labels = np.argmax(test_labels, axis=1)

cm = confusion_matrix(test_img_pred_labels, test_true_labels)

ConfusionMatrixDisplay(confusion_matrix=cm, 
                       display_labels=['Neg-Pneumonia', 'Pos-Pneumonia']).plot(ax=ax, cmap='Blues', colorbar=False)

plt.tight_layout();









